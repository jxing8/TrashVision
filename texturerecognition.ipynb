{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"texturerecognition.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"JsH69sBEVU7h","colab_type":"code","outputId":"ef3d6d17-f45d-4230-88fd-875781691591","executionInfo":{"status":"ok","timestamp":1574826381253,"user_tz":480,"elapsed":35084,"user":{"displayName":"Khanh Nguyen","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAhmdzYE4Np2rwuKzC_ZuN37tvzMf_2DpvMfuRD=s64","userId":"04394352403186034085"}},"colab":{"base_uri":"https://localhost:8080/","height":125}},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0Lf4XhZrLFxv","colab_type":"code","colab":{}},"source":["!unzip -q drive/'My Drive'/dataset-original.zip"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"v-IQewQhBOfA","colab_type":"code","outputId":"5bf71156-7163-4447-c95d-2eec45719ec1","executionInfo":{"status":"ok","timestamp":1574826455313,"user_tz":480,"elapsed":109124,"user":{"displayName":"Khanh Nguyen","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAhmdzYE4Np2rwuKzC_ZuN37tvzMf_2DpvMfuRD=s64","userId":"04394352403186034085"}},"colab":{"base_uri":"https://localhost:8080/","height":439}},"source":["!mkdir dataset-validation\n","!mkdir dataset-validation/cardboard\n","!mkdir dataset-validation/glass\n","!mkdir dataset-validation/paper\n","!mkdir dataset-validation/plastic\n","!mkdir dataset-validation/metal\n","!mkdir dataset-validation/trash\n","!ls\n","!ls dataset-original/trash"],"execution_count":0,"outputs":[{"output_type":"stream","text":["dataset-original  dataset-validation  drive  __MACOSX  sample_data\n","trash100.jpg  trash121.jpg  trash18.jpg  trash39.jpg  trash5.jpg   trash80.jpg\n","trash101.jpg  trash122.jpg  trash19.jpg  trash3.jpg   trash60.jpg  trash81.jpg\n","trash102.jpg  trash123.jpg  trash1.jpg\t trash40.jpg  trash61.jpg  trash82.jpg\n","trash103.jpg  trash124.jpg  trash20.jpg  trash41.jpg  trash62.jpg  trash83.jpg\n","trash104.jpg  trash125.jpg  trash21.jpg  trash42.jpg  trash63.jpg  trash84.jpg\n","trash105.jpg  trash126.jpg  trash22.jpg  trash43.jpg  trash64.jpg  trash85.jpg\n","trash106.jpg  trash127.jpg  trash23.jpg  trash44.jpg  trash65.jpg  trash86.jpg\n","trash107.jpg  trash128.jpg  trash24.jpg  trash45.jpg  trash66.jpg  trash87.jpg\n","trash108.jpg  trash129.jpg  trash25.jpg  trash46.jpg  trash67.jpg  trash88.jpg\n","trash109.jpg  trash12.jpg   trash26.jpg  trash47.jpg  trash68.jpg  trash89.jpg\n","trash10.jpg   trash130.jpg  trash27.jpg  trash48.jpg  trash69.jpg  trash8.jpg\n","trash110.jpg  trash131.jpg  trash28.jpg  trash49.jpg  trash6.jpg   trash90.jpg\n","trash111.jpg  trash132.jpg  trash29.jpg  trash4.jpg   trash70.jpg  trash91.jpg\n","trash112.jpg  trash133.jpg  trash2.jpg\t trash50.jpg  trash71.jpg  trash92.jpg\n","trash113.jpg  trash134.jpg  trash30.jpg  trash51.jpg  trash72.jpg  trash93.jpg\n","trash114.jpg  trash135.jpg  trash31.jpg  trash52.jpg  trash73.jpg  trash94.jpg\n","trash115.jpg  trash136.jpg  trash32.jpg  trash53.jpg  trash74.jpg  trash95.jpg\n","trash116.jpg  trash137.jpg  trash33.jpg  trash54.jpg  trash75.jpg  trash96.jpg\n","trash117.jpg  trash13.jpg   trash34.jpg  trash55.jpg  trash76.jpg  trash97.jpg\n","trash118.jpg  trash14.jpg   trash35.jpg  trash56.jpg  trash77.jpg  trash98.jpg\n","trash119.jpg  trash15.jpg   trash36.jpg  trash57.jpg  trash78.jpg  trash99.jpg\n","trash11.jpg   trash16.jpg   trash37.jpg  trash58.jpg  trash79.jpg  trash9.jpg\n","trash120.jpg  trash17.jpg   trash38.jpg  trash59.jpg  trash7.jpg\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vK4hhHg0FNiC","colab_type":"code","colab":{}},"source":["!shuf -n 101 -e dataset-original/glass/* | xargs -i mv {} dataset-validation/glass/\n","!shuf -n 144 -e dataset-original/paper/* | xargs -i mv {} dataset-validation/paper/\n","!shuf -n 53 -e dataset-original/cardboard/* | xargs -i mv {} dataset-validation/cardboard/\n","!shuf -n 82 -e dataset-original/plastic/* | xargs -i mv {} dataset-validation/plastic/\n","!shuf -n 60 -e dataset-original/metal/* | xargs -i mv {} dataset-validation/metal/\n","!shuf -n 37 -e dataset-original/trash/* | xargs -i mv {} dataset-validation/trash/"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uFV1fTT9HspD","colab_type":"code","outputId":"45b383dc-8096-4077-a051-67383dbc8d95","executionInfo":{"status":"ok","timestamp":1574826469181,"user_tz":480,"elapsed":122978,"user":{"displayName":"Khanh Nguyen","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAhmdzYE4Np2rwuKzC_ZuN37tvzMf_2DpvMfuRD=s64","userId":"04394352403186034085"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["!ls dataset-original/glass | wc -l\n","!ls dataset-validation/glass | wc -l"],"execution_count":0,"outputs":[{"output_type":"stream","text":["400\n","101\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VCMDyL_T7fdA","colab_type":"code","colab":{}},"source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","import torch.nn as nn\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"i6wBY7KQYiTN","colab_type":"code","colab":{}},"source":["transformationsTrain = transforms.Compose([transforms.Resize(255), transforms.RandomHorizontalFlip(), transforms.RandomVerticalFlip(),\n","                                           transforms.RandomRotation(degrees = (90, -90)), transforms.ToTensor(), \n","                                           transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","transformationsValidation = transforms.Compose([transforms.Resize(255), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zDud-z_t-7QK","colab_type":"code","colab":{}},"source":["train_set = datasets.ImageFolder(\"dataset-original\", transform = transformationsTrain)\n","val_set = datasets.ImageFolder(\"dataset-validation\", transform = transformationsValidation)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ICFzgBLtdiwC","colab_type":"code","colab":{}},"source":["train_loader = torch.utils.data.DataLoader(train_set, batch_size=50, shuffle=True)\n","val_loader = torch.utils.data.DataLoader(val_set, batch_size =50, shuffle=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"u3TUtmB4LheG","colab_type":"code","outputId":"0ed2c8f4-3855-44c6-d90f-5ebe2977add1","executionInfo":{"status":"ok","timestamp":1574807586200,"user_tz":480,"elapsed":875,"user":{"displayName":"Khanh Nguyen","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAhmdzYE4Np2rwuKzC_ZuN37tvzMf_2DpvMfuRD=s64","userId":"04394352403186034085"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["model = torchvision.models.resnet34()\n","model.cuda()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (2): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (2): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (3): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (2): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (3): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (4): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (5): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (2): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"SWIqG7BNLuXi","colab_type":"code","colab":{}},"source":["learningRate = 1e-2\n","epochs = 5\n","minibatches = 5"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aFM3s_k62J7F","colab_type":"code","outputId":"ff231430-4241-44be-bf08-b5c39fb96228","executionInfo":{"status":"ok","timestamp":1574810073933,"user_tz":480,"elapsed":2481241,"user":{"displayName":"Khanh Nguyen","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAhmdzYE4Np2rwuKzC_ZuN37tvzMf_2DpvMfuRD=s64","userId":"04394352403186034085"}},"colab":{"base_uri":"https://localhost:8080/","height":827}},"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), learningRate)\n","\n","train_losses, train_accs = [], []\n","val_losses, val_accs = [], []\n","\n","for epoch in range(epochs):  # loop over the dataset multiple times\n","\n","    running_loss = 0.0\n","    model.train()\n","    for i, data in enumerate(train_loader, 0):\n","        # get the inputs; data is a list of [inputs, labels]\n","        inputs, labels = data\n","        inputs = inputs.cuda()\n","        labels = labels.cuda()\n","\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs = model(inputs)\n","\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        pred = outputs.data.max(1)[1]\n","        matches = labels == pred\n","        accuracy = matches.float().mean()\n","\n","        # print statistics\n","        running_loss += loss.item()\n","        if i % minibatches == minibatches - 1:    # print every n mini-batches\n","            print('[%d, %5d] loss: %.3f accuracy: %.3f itemLoss: %.3f' %\n","                  (epoch + 1, i + 1, running_loss / minibatches, accuracy, loss.item()))\n","            running_loss = 0.0\n","\n","        train_losses.append(loss.item())\n","        train_accs.append(accuracy.item())\n","    \n","    model.eval()\n","    val_loss, correct = 0., 0\n","    total = 0\n","    with torch.no_grad():\n","      for data in val_loader:\n","        inputs, labels = data\n","        inputs = inputs.cuda()\n","        labels = labels.cuda()\n","        outputs = model(inputs)\n","        val_loss += criterion(outputs, labels).item()\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","    print('Accuracy of the network on the validation images: %d %%' % (\n","    100 * correct / total))\n","\n","    val_loss /= len(val_loader)\n","    acc = correct / len(val_loader)\n","\n","    val_losses.append(val_loss)\n","    val_accs.append(acc)\n","\n","print('Finished Training')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[1,     5] loss: 4.889 accuracy: 0.240 itemLoss: 3.080\n","[1,    10] loss: 2.466 accuracy: 0.240 itemLoss: 2.243\n","[1,    15] loss: 2.034 accuracy: 0.240 itemLoss: 2.033\n","[1,    20] loss: 1.887 accuracy: 0.360 itemLoss: 1.641\n","[1,    25] loss: 1.772 accuracy: 0.240 itemLoss: 1.794\n","[1,    30] loss: 1.667 accuracy: 0.300 itemLoss: 1.711\n","[1,    35] loss: 1.638 accuracy: 0.400 itemLoss: 1.621\n","[1,    40] loss: 1.636 accuracy: 0.360 itemLoss: 1.789\n","Accuracy of the network on the validation images: 34 %\n","[2,     5] loss: 1.552 accuracy: 0.420 itemLoss: 1.485\n","[2,    10] loss: 1.483 accuracy: 0.600 itemLoss: 1.204\n","[2,    15] loss: 1.392 accuracy: 0.380 itemLoss: 1.388\n","[2,    20] loss: 1.473 accuracy: 0.420 itemLoss: 1.541\n","[2,    25] loss: 1.486 accuracy: 0.500 itemLoss: 1.256\n","[2,    30] loss: 1.360 accuracy: 0.340 itemLoss: 1.581\n","[2,    35] loss: 1.450 accuracy: 0.320 itemLoss: 1.447\n","[2,    40] loss: 1.481 accuracy: 0.400 itemLoss: 1.761\n","Accuracy of the network on the validation images: 33 %\n","[3,     5] loss: 1.478 accuracy: 0.480 itemLoss: 1.330\n","[3,    10] loss: 1.416 accuracy: 0.520 itemLoss: 1.250\n","[3,    15] loss: 1.284 accuracy: 0.460 itemLoss: 1.245\n","[3,    20] loss: 1.275 accuracy: 0.440 itemLoss: 1.299\n","[3,    25] loss: 1.423 accuracy: 0.440 itemLoss: 1.411\n","[3,    30] loss: 1.326 accuracy: 0.400 itemLoss: 1.330\n","[3,    35] loss: 1.286 accuracy: 0.460 itemLoss: 1.483\n","[3,    40] loss: 1.287 accuracy: 0.620 itemLoss: 1.065\n","Accuracy of the network on the validation images: 51 %\n","[4,     5] loss: 1.309 accuracy: 0.500 itemLoss: 1.325\n","[4,    10] loss: 1.250 accuracy: 0.460 itemLoss: 1.313\n","[4,    15] loss: 1.314 accuracy: 0.520 itemLoss: 1.278\n","[4,    20] loss: 1.299 accuracy: 0.620 itemLoss: 1.076\n","[4,    25] loss: 1.362 accuracy: 0.440 itemLoss: 1.418\n","[4,    30] loss: 1.226 accuracy: 0.540 itemLoss: 1.195\n","[4,    35] loss: 1.180 accuracy: 0.640 itemLoss: 1.111\n","[4,    40] loss: 1.248 accuracy: 0.540 itemLoss: 1.116\n","Accuracy of the network on the validation images: 47 %\n","[5,     5] loss: 1.129 accuracy: 0.540 itemLoss: 1.158\n","[5,    10] loss: 1.271 accuracy: 0.500 itemLoss: 1.258\n","[5,    15] loss: 1.211 accuracy: 0.540 itemLoss: 1.246\n","[5,    20] loss: 1.203 accuracy: 0.500 itemLoss: 1.418\n","[5,    25] loss: 1.197 accuracy: 0.580 itemLoss: 1.173\n","[5,    30] loss: 1.157 accuracy: 0.540 itemLoss: 1.129\n","[5,    35] loss: 1.067 accuracy: 0.460 itemLoss: 1.320\n","[5,    40] loss: 1.254 accuracy: 0.540 itemLoss: 1.311\n","Accuracy of the network on the validation images: 50 %\n","Finished Training\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6RupYgizctQo","colab_type":"code","colab":{}},"source":["torch.save(model.state_dict(), \"/content/drive/My Drive/modelNov26.pt\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YZ30FVHqj7Zp","colab_type":"code","outputId":"81fb1b94-56c8-49eb-c3fd-dd9cbcf6b14e","executionInfo":{"status":"ok","timestamp":1574802894805,"user_tz":480,"elapsed":10161,"user":{"displayName":"Khanh Nguyen","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAhmdzYE4Np2rwuKzC_ZuN37tvzMf_2DpvMfuRD=s64","userId":"04394352403186034085"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# model = torchvision.models.resnet34()\n","# model.cuda()\n","# model.load_state_dict(torch.load(\"/content/drive/My Drive/modelNov25.pt\"))"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"j2361SA_kT4O","colab_type":"code","outputId":"74397c7a-b9eb-4e4e-a368-c342b196edcc","executionInfo":{"status":"ok","timestamp":1574812536253,"user_tz":480,"elapsed":2462283,"user":{"displayName":"Khanh Nguyen","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAhmdzYE4Np2rwuKzC_ZuN37tvzMf_2DpvMfuRD=s64","userId":"04394352403186034085"}},"colab":{"base_uri":"https://localhost:8080/","height":862}},"source":["model = torchvision.models.resnet34(pretrained=True)\n","model.cuda()\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), learningRate)\n","\n","train_losses, train_accs = [], []\n","val_losses, val_accs = [], []\n","\n","for epoch in range(epochs):  # loop over the dataset multiple times\n","\n","    running_loss = 0.0\n","    model.train()\n","    for i, data in enumerate(train_loader, 0):\n","        # get the inputs; data is a list of [inputs, labels]\n","        inputs, labels = data\n","        inputs = inputs.cuda()\n","        labels = labels.cuda()\n","\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs = model(inputs)\n","\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        pred = outputs.data.max(1)[1]\n","        matches = labels == pred\n","        accuracy = matches.float().mean()\n","\n","        # print statistics\n","        running_loss += loss.item()\n","        if i % minibatches == minibatches - 1:    # print every n mini-batches\n","            print('[%d, %5d] loss: %.3f accuracy: %.3f itemLoss: %.3f' %\n","                  (epoch + 1, i + 1, running_loss / minibatches, accuracy, loss.item()))\n","            running_loss = 0.0\n","\n","        train_losses.append(loss.item())\n","        train_accs.append(accuracy.item())\n","    \n","    model.eval()\n","    val_loss, correct = 0., 0\n","    total = 0\n","    with torch.no_grad():\n","      for data in val_loader:\n","        inputs, labels = data\n","        inputs = inputs.cuda()\n","        labels = labels.cuda()\n","        outputs = model(inputs)\n","        val_loss += criterion(outputs, labels).item()\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","    print('Accuracy of the network on the validation images: %d %%' % (\n","    100 * correct / total))\n","\n","    val_loss /= len(val_loader)\n","    acc = correct / len(val_loader)\n","\n","    val_losses.append(val_loss)\n","    val_accs.append(acc)\n","\n","print('Finished Training')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /root/.cache/torch/checkpoints/resnet34-333f7ec4.pth\n","100%|██████████| 83.3M/83.3M [00:00<00:00, 160MB/s] \n"],"name":"stderr"},{"output_type":"stream","text":["[1,     5] loss: 7.152 accuracy: 0.080 itemLoss: 4.820\n","[1,    10] loss: 2.577 accuracy: 0.660 itemLoss: 1.441\n","[1,    15] loss: 1.525 accuracy: 0.580 itemLoss: 1.391\n","[1,    20] loss: 1.054 accuracy: 0.820 itemLoss: 0.753\n","[1,    25] loss: 1.001 accuracy: 0.700 itemLoss: 1.041\n","[1,    30] loss: 0.908 accuracy: 0.760 itemLoss: 0.837\n","[1,    35] loss: 0.727 accuracy: 0.780 itemLoss: 0.702\n","[1,    40] loss: 0.711 accuracy: 0.860 itemLoss: 0.462\n","Accuracy of the network on the validation images: 76 %\n","[2,     5] loss: 0.559 accuracy: 0.860 itemLoss: 0.442\n","[2,    10] loss: 0.592 accuracy: 0.820 itemLoss: 0.596\n","[2,    15] loss: 0.547 accuracy: 0.820 itemLoss: 0.466\n","[2,    20] loss: 0.571 accuracy: 0.840 itemLoss: 0.579\n","[2,    25] loss: 0.467 accuracy: 0.740 itemLoss: 0.753\n","[2,    30] loss: 0.478 accuracy: 0.840 itemLoss: 0.527\n","[2,    35] loss: 0.345 accuracy: 0.840 itemLoss: 0.405\n","[2,    40] loss: 0.456 accuracy: 0.820 itemLoss: 0.434\n","Accuracy of the network on the validation images: 87 %\n","[3,     5] loss: 0.416 accuracy: 0.920 itemLoss: 0.312\n","[3,    10] loss: 0.337 accuracy: 0.860 itemLoss: 0.378\n","[3,    15] loss: 0.369 accuracy: 0.860 itemLoss: 0.569\n","[3,    20] loss: 0.389 accuracy: 0.880 itemLoss: 0.368\n","[3,    25] loss: 0.400 accuracy: 0.900 itemLoss: 0.285\n","[3,    30] loss: 0.286 accuracy: 0.820 itemLoss: 0.535\n","[3,    35] loss: 0.395 accuracy: 0.800 itemLoss: 0.646\n","[3,    40] loss: 0.340 accuracy: 0.900 itemLoss: 0.320\n","Accuracy of the network on the validation images: 87 %\n","[4,     5] loss: 0.249 accuracy: 1.000 itemLoss: 0.118\n","[4,    10] loss: 0.258 accuracy: 0.900 itemLoss: 0.290\n","[4,    15] loss: 0.239 accuracy: 0.880 itemLoss: 0.234\n","[4,    20] loss: 0.277 accuracy: 0.880 itemLoss: 0.304\n","[4,    25] loss: 0.212 accuracy: 0.980 itemLoss: 0.146\n","[4,    30] loss: 0.204 accuracy: 0.960 itemLoss: 0.172\n","[4,    35] loss: 0.326 accuracy: 0.820 itemLoss: 0.507\n","[4,    40] loss: 0.239 accuracy: 0.960 itemLoss: 0.148\n","Accuracy of the network on the validation images: 87 %\n","[5,     5] loss: 0.220 accuracy: 0.880 itemLoss: 0.279\n","[5,    10] loss: 0.196 accuracy: 0.940 itemLoss: 0.178\n","[5,    15] loss: 0.245 accuracy: 0.940 itemLoss: 0.179\n","[5,    20] loss: 0.283 accuracy: 0.940 itemLoss: 0.245\n","[5,    25] loss: 0.187 accuracy: 0.960 itemLoss: 0.156\n","[5,    30] loss: 0.187 accuracy: 0.940 itemLoss: 0.209\n","[5,    35] loss: 0.224 accuracy: 0.940 itemLoss: 0.203\n","[5,    40] loss: 0.230 accuracy: 0.920 itemLoss: 0.185\n","Accuracy of the network on the validation images: 89 %\n","Finished Training\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uc35vxVl8j-Z","colab_type":"code","outputId":"16ecfafa-747a-4d8a-b2bc-6bac80d20f02","executionInfo":{"status":"ok","timestamp":1574829741481,"user_tz":480,"elapsed":232473,"user":{"displayName":"Khanh Nguyen","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAhmdzYE4Np2rwuKzC_ZuN37tvzMf_2DpvMfuRD=s64","userId":"04394352403186034085"}},"colab":{"base_uri":"https://localhost:8080/","height":862}},"source":["model = torchvision.models.resnet34(pretrained=True)\n","model.cuda()\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), learningRate)\n","\n","train_losses, train_accs = [], []\n","val_losses, val_accs = [], []\n","\n","for epoch in range(epochs):  # loop over the dataset multiple times\n","\n","    running_loss = 0.0\n","    model.train()\n","    for i, data in enumerate(train_loader, 0):\n","        # get the inputs; data is a list of [inputs, labels]\n","        inputs, labels = data\n","        inputs = inputs.cuda()\n","        labels = labels.cuda()\n","\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs = model(inputs)\n","\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        pred = outputs.data.max(1)[1]\n","        matches = labels == pred\n","        accuracy = matches.float().mean()\n","\n","        # print statistics\n","        running_loss += loss.item()\n","        if i % minibatches == minibatches - 1:    # print every n mini-batches\n","            print('[%d, %5d] loss: %.3f accuracy: %.3f itemLoss: %.3f' %\n","                  (epoch + 1, i + 1, running_loss / minibatches, accuracy, loss.item()))\n","            running_loss = 0.0\n","\n","        train_losses.append(loss.item())\n","        train_accs.append(accuracy.item())\n","    \n","    model.eval()\n","    val_loss, correct = 0., 0\n","    total = 0\n","    with torch.no_grad():\n","      for data in val_loader:\n","        inputs, labels = data\n","        inputs = inputs.cuda()\n","        labels = labels.cuda()\n","        outputs = model(inputs)\n","        val_loss += criterion(outputs, labels).item()\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","    print('Accuracy of the network on the validation images: %d %%' % (\n","    100 * correct / total))\n","\n","    val_loss /= len(val_loader)\n","    acc = correct / len(val_loader)\n","\n","    val_losses.append(val_loss)\n","    val_accs.append(acc)\n","\n","print('Finished Training')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /root/.cache/torch/checkpoints/resnet34-333f7ec4.pth\n","100%|██████████| 83.3M/83.3M [00:00<00:00, 106MB/s]\n"],"name":"stderr"},{"output_type":"stream","text":["[1,     5] loss: 4.935 accuracy: 0.180 itemLoss: 2.417\n","[1,    10] loss: 2.181 accuracy: 0.240 itemLoss: 2.429\n","[1,    15] loss: 1.820 accuracy: 0.300 itemLoss: 1.686\n","[1,    20] loss: 1.784 accuracy: 0.280 itemLoss: 2.778\n","[1,    25] loss: 1.981 accuracy: 0.320 itemLoss: 1.869\n","[1,    30] loss: 1.950 accuracy: 0.340 itemLoss: 2.015\n","[1,    35] loss: 1.655 accuracy: 0.380 itemLoss: 1.600\n","[1,    40] loss: 1.553 accuracy: 0.360 itemLoss: 1.349\n","Accuracy of the network on the validation images: 28 %\n","[2,     5] loss: 1.950 accuracy: 0.340 itemLoss: 1.573\n","[2,    10] loss: 1.817 accuracy: 0.400 itemLoss: 1.928\n","[2,    15] loss: 1.660 accuracy: 0.360 itemLoss: 1.630\n","[2,    20] loss: 1.515 accuracy: 0.360 itemLoss: 1.470\n","[2,    25] loss: 1.370 accuracy: 0.500 itemLoss: 1.210\n","[2,    30] loss: 1.491 accuracy: 0.400 itemLoss: 1.539\n","[2,    35] loss: 1.620 accuracy: 0.320 itemLoss: 1.937\n","[2,    40] loss: 1.419 accuracy: 0.420 itemLoss: 1.529\n","Accuracy of the network on the validation images: 34 %\n","[3,     5] loss: 1.643 accuracy: 0.480 itemLoss: 1.403\n","[3,    10] loss: 1.334 accuracy: 0.580 itemLoss: 1.133\n","[3,    15] loss: 1.360 accuracy: 0.340 itemLoss: 1.575\n","[3,    20] loss: 1.439 accuracy: 0.520 itemLoss: 1.345\n","[3,    25] loss: 1.400 accuracy: 0.440 itemLoss: 1.427\n","[3,    30] loss: 1.425 accuracy: 0.420 itemLoss: 1.425\n","[3,    35] loss: 1.555 accuracy: 0.500 itemLoss: 1.376\n","[3,    40] loss: 1.488 accuracy: 0.500 itemLoss: 1.359\n","Accuracy of the network on the validation images: 51 %\n","[4,     5] loss: 1.454 accuracy: 0.460 itemLoss: 1.467\n","[4,    10] loss: 1.414 accuracy: 0.400 itemLoss: 1.599\n","[4,    15] loss: 1.478 accuracy: 0.440 itemLoss: 1.392\n","[4,    20] loss: 1.379 accuracy: 0.480 itemLoss: 1.327\n","[4,    25] loss: 1.511 accuracy: 0.460 itemLoss: 1.617\n","[4,    30] loss: 1.451 accuracy: 0.440 itemLoss: 1.693\n","[4,    35] loss: 1.349 accuracy: 0.360 itemLoss: 1.457\n","[4,    40] loss: 1.363 accuracy: 0.440 itemLoss: 1.496\n","Accuracy of the network on the validation images: 31 %\n","[5,     5] loss: 1.641 accuracy: 0.480 itemLoss: 1.277\n","[5,    10] loss: 1.641 accuracy: 0.320 itemLoss: 1.501\n","[5,    15] loss: 1.739 accuracy: 0.380 itemLoss: 1.574\n","[5,    20] loss: 1.568 accuracy: 0.420 itemLoss: 2.010\n","[5,    25] loss: 1.479 accuracy: 0.500 itemLoss: 1.185\n","[5,    30] loss: 1.311 accuracy: 0.500 itemLoss: 1.300\n","[5,    35] loss: 1.402 accuracy: 0.400 itemLoss: 1.502\n","[5,    40] loss: 1.548 accuracy: 0.520 itemLoss: 1.431\n","Accuracy of the network on the validation images: 34 %\n","Finished Training\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cH5TF1GlB3qB","colab_type":"code","outputId":"7a6c5f9a-f167-4742-955c-a4be756b5001","executionInfo":{"status":"ok","timestamp":1574832974745,"user_tz":480,"elapsed":3451825,"user":{"displayName":"Khanh Nguyen","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAhmdzYE4Np2rwuKzC_ZuN37tvzMf_2DpvMfuRD=s64","userId":"04394352403186034085"}},"colab":{"base_uri":"https://localhost:8080/","height":827}},"source":["model = torchvision.models.resnet34(pretrained=True)\n","model.cuda()\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n","\n","train_losses, train_accs = [], []\n","val_losses, val_accs = [], []\n","\n","for epoch in range(epochs):  # loop over the dataset multiple times\n","\n","    running_loss = 0.0\n","    model.train()\n","    for i, data in enumerate(train_loader, 0):\n","        # get the inputs; data is a list of [inputs, labels]\n","        inputs, labels = data\n","        inputs = inputs.cuda()\n","        labels = labels.cuda()\n","\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs = model(inputs)\n","\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        pred = outputs.data.max(1)[1]\n","        matches = labels == pred\n","        accuracy = matches.float().mean()\n","\n","        # print statistics\n","        running_loss += loss.item()\n","        if i % minibatches == minibatches - 1:    # print every n mini-batches\n","            print('[%d, %5d] loss: %.3f accuracy: %.3f itemLoss: %.3f' %\n","                  (epoch + 1, i + 1, running_loss / minibatches, accuracy, loss.item()))\n","            running_loss = 0.0\n","\n","        train_losses.append(loss.item())\n","        train_accs.append(accuracy.item())\n","    \n","    model.eval()\n","    val_loss, correct = 0., 0\n","    total = 0\n","    with torch.no_grad():\n","      for data in val_loader:\n","        inputs, labels = data\n","        inputs = inputs.cuda()\n","        labels = labels.cuda()\n","        outputs = model(inputs)\n","        val_loss += criterion(outputs, labels).item()\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","    print('Accuracy of the network on the validation images: %d %%' % (\n","    100 * correct / total))\n","\n","    val_loss /= len(val_loader)\n","    acc = correct / len(val_loader)\n","\n","    val_losses.append(val_loss)\n","    val_accs.append(acc)\n","\n","print('Finished Training')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[1,     5] loss: 4.520 accuracy: 0.740 itemLoss: 1.642\n","[1,    10] loss: 1.870 accuracy: 0.440 itemLoss: 2.105\n","[1,    15] loss: 1.459 accuracy: 0.480 itemLoss: 1.806\n","[1,    20] loss: 1.226 accuracy: 0.640 itemLoss: 1.219\n","[1,    25] loss: 1.096 accuracy: 0.660 itemLoss: 1.006\n","[1,    30] loss: 0.903 accuracy: 0.640 itemLoss: 1.008\n","[1,    35] loss: 1.104 accuracy: 0.580 itemLoss: 1.161\n","[1,    40] loss: 1.012 accuracy: 0.620 itemLoss: 1.240\n","Accuracy of the network on the validation images: 62 %\n","[2,     5] loss: 0.974 accuracy: 0.600 itemLoss: 1.107\n","[2,    10] loss: 0.836 accuracy: 0.680 itemLoss: 0.822\n","[2,    15] loss: 0.835 accuracy: 0.760 itemLoss: 0.712\n","[2,    20] loss: 0.771 accuracy: 0.660 itemLoss: 0.701\n","[2,    25] loss: 0.713 accuracy: 0.700 itemLoss: 0.739\n","[2,    30] loss: 0.807 accuracy: 0.740 itemLoss: 0.663\n","[2,    35] loss: 0.633 accuracy: 0.760 itemLoss: 0.701\n","[2,    40] loss: 0.661 accuracy: 0.760 itemLoss: 0.569\n","Accuracy of the network on the validation images: 73 %\n","[3,     5] loss: 0.677 accuracy: 0.820 itemLoss: 0.510\n","[3,    10] loss: 0.707 accuracy: 0.880 itemLoss: 0.628\n","[3,    15] loss: 0.585 accuracy: 0.740 itemLoss: 0.752\n","[3,    20] loss: 0.638 accuracy: 0.700 itemLoss: 0.691\n","[3,    25] loss: 0.672 accuracy: 0.740 itemLoss: 0.653\n","[3,    30] loss: 0.741 accuracy: 0.800 itemLoss: 0.535\n","[3,    35] loss: 0.691 accuracy: 0.720 itemLoss: 0.744\n","[3,    40] loss: 0.707 accuracy: 0.740 itemLoss: 0.670\n","Accuracy of the network on the validation images: 61 %\n","[4,     5] loss: 0.511 accuracy: 0.740 itemLoss: 0.649\n","[4,    10] loss: 0.537 accuracy: 0.760 itemLoss: 0.623\n","[4,    15] loss: 0.673 accuracy: 0.840 itemLoss: 0.539\n","[4,    20] loss: 0.705 accuracy: 0.740 itemLoss: 0.618\n","[4,    25] loss: 0.644 accuracy: 0.820 itemLoss: 0.678\n","[4,    30] loss: 0.524 accuracy: 0.860 itemLoss: 0.309\n","[4,    35] loss: 0.602 accuracy: 0.760 itemLoss: 0.725\n","[4,    40] loss: 0.567 accuracy: 0.820 itemLoss: 0.447\n","Accuracy of the network on the validation images: 77 %\n","[5,     5] loss: 0.510 accuracy: 0.860 itemLoss: 0.398\n","[5,    10] loss: 0.501 accuracy: 0.860 itemLoss: 0.349\n","[5,    15] loss: 0.511 accuracy: 0.880 itemLoss: 0.366\n","[5,    20] loss: 0.416 accuracy: 0.820 itemLoss: 0.491\n","[5,    25] loss: 0.506 accuracy: 0.800 itemLoss: 0.577\n","[5,    30] loss: 0.417 accuracy: 0.820 itemLoss: 0.423\n","[5,    35] loss: 0.554 accuracy: 0.920 itemLoss: 0.400\n","[5,    40] loss: 0.552 accuracy: 0.860 itemLoss: 0.554\n","Accuracy of the network on the validation images: 80 %\n","Finished Training\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Aebz65-wCYWl","colab_type":"code","outputId":"f2b9e60a-b3b5-48cf-ed6f-d1a5ecb59697","executionInfo":{"status":"ok","timestamp":1574832980421,"user_tz":480,"elapsed":3453715,"user":{"displayName":"Khanh Nguyen","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAhmdzYE4Np2rwuKzC_ZuN37tvzMf_2DpvMfuRD=s64","userId":"04394352403186034085"}},"colab":{"base_uri":"https://localhost:8080/","height":123}},"source":["!pip install adabound\n","\n","import adabound"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting adabound\n","  Downloading https://files.pythonhosted.org/packages/cd/44/0c2c414effb3d9750d780b230dbb67ea48ddc5d9a6d7a9b7e6fcc6bdcff9/adabound-0.0.5-py3-none-any.whl\n","Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from adabound) (1.3.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.0->adabound) (1.17.4)\n","Installing collected packages: adabound\n","Successfully installed adabound-0.0.5\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EjMPAuiVCVTm","colab_type":"code","outputId":"d91355a6-0e6b-46b8-e7b3-67d40ac4a3d1","executionInfo":{"status":"ok","timestamp":1574836219601,"user_tz":480,"elapsed":6691889,"user":{"displayName":"Khanh Nguyen","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAhmdzYE4Np2rwuKzC_ZuN37tvzMf_2DpvMfuRD=s64","userId":"04394352403186034085"}},"colab":{"base_uri":"https://localhost:8080/","height":827}},"source":["model = torchvision.models.resnet34(pretrained=True)\n","model.cuda()\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=0.1)\n","\n","train_losses, train_accs = [], []\n","val_losses, val_accs = [], []\n","\n","for epoch in range(epochs):  # loop over the dataset multiple times\n","\n","    running_loss = 0.0\n","    model.train()\n","    for i, data in enumerate(train_loader, 0):\n","        # get the inputs; data is a list of [inputs, labels]\n","        inputs, labels = data\n","        inputs = inputs.cuda()\n","        labels = labels.cuda()\n","\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs = model(inputs)\n","\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        pred = outputs.data.max(1)[1]\n","        matches = labels == pred\n","        accuracy = matches.float().mean()\n","\n","        # print statistics\n","        running_loss += loss.item()\n","        if i % minibatches == minibatches - 1:    # print every n mini-batches\n","            print('[%d, %5d] loss: %.3f accuracy: %.3f itemLoss: %.3f' %\n","                  (epoch + 1, i + 1, running_loss / minibatches, accuracy, loss.item()))\n","            running_loss = 0.0\n","\n","        train_losses.append(loss.item())\n","        train_accs.append(accuracy.item())\n","    \n","    model.eval()\n","    val_loss, correct = 0., 0\n","    total = 0\n","    with torch.no_grad():\n","      for data in val_loader:\n","        inputs, labels = data\n","        inputs = inputs.cuda()\n","        labels = labels.cuda()\n","        outputs = model(inputs)\n","        val_loss += criterion(outputs, labels).item()\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","    print('Accuracy of the network on the validation images: %d %%' % (\n","    100 * correct / total))\n","\n","    val_loss /= len(val_loader)\n","    acc = correct / len(val_loader)\n","\n","    val_losses.append(val_loss)\n","    val_accs.append(acc)\n","\n","print('Finished Training')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[1,     5] loss: 4.873 accuracy: 0.500 itemLoss: 3.133\n","[1,    10] loss: 2.033 accuracy: 0.620 itemLoss: 1.243\n","[1,    15] loss: 1.351 accuracy: 0.600 itemLoss: 1.349\n","[1,    20] loss: 1.074 accuracy: 0.660 itemLoss: 1.006\n","[1,    25] loss: 1.097 accuracy: 0.620 itemLoss: 1.076\n","[1,    30] loss: 1.081 accuracy: 0.640 itemLoss: 0.882\n","[1,    35] loss: 0.842 accuracy: 0.740 itemLoss: 0.726\n","[1,    40] loss: 0.859 accuracy: 0.560 itemLoss: 0.949\n","Accuracy of the network on the validation images: 57 %\n","[2,     5] loss: 0.744 accuracy: 0.820 itemLoss: 0.540\n","[2,    10] loss: 0.723 accuracy: 0.720 itemLoss: 0.657\n","[2,    15] loss: 0.792 accuracy: 0.680 itemLoss: 0.742\n","[2,    20] loss: 0.683 accuracy: 0.700 itemLoss: 0.933\n","[2,    25] loss: 0.805 accuracy: 0.640 itemLoss: 1.108\n","[2,    30] loss: 0.776 accuracy: 0.680 itemLoss: 0.792\n","[2,    35] loss: 0.629 accuracy: 0.800 itemLoss: 0.685\n","[2,    40] loss: 0.726 accuracy: 0.640 itemLoss: 0.729\n","Accuracy of the network on the validation images: 70 %\n","[3,     5] loss: 0.590 accuracy: 0.820 itemLoss: 0.581\n","[3,    10] loss: 0.574 accuracy: 0.860 itemLoss: 0.449\n","[3,    15] loss: 0.663 accuracy: 0.800 itemLoss: 0.732\n","[3,    20] loss: 0.576 accuracy: 0.860 itemLoss: 0.419\n","[3,    25] loss: 0.554 accuracy: 0.820 itemLoss: 0.436\n","[3,    30] loss: 0.703 accuracy: 0.660 itemLoss: 0.842\n","[3,    35] loss: 0.624 accuracy: 0.800 itemLoss: 0.491\n","[3,    40] loss: 0.604 accuracy: 0.840 itemLoss: 0.618\n","Accuracy of the network on the validation images: 70 %\n","[4,     5] loss: 0.557 accuracy: 0.820 itemLoss: 0.629\n","[4,    10] loss: 0.487 accuracy: 0.860 itemLoss: 0.497\n","[4,    15] loss: 0.645 accuracy: 0.800 itemLoss: 0.759\n","[4,    20] loss: 0.454 accuracy: 0.840 itemLoss: 0.462\n","[4,    25] loss: 0.450 accuracy: 0.880 itemLoss: 0.497\n","[4,    30] loss: 0.358 accuracy: 0.900 itemLoss: 0.292\n","[4,    35] loss: 0.506 accuracy: 0.840 itemLoss: 0.456\n","[4,    40] loss: 0.489 accuracy: 0.880 itemLoss: 0.355\n","Accuracy of the network on the validation images: 73 %\n","[5,     5] loss: 0.422 accuracy: 0.820 itemLoss: 0.552\n","[5,    10] loss: 0.550 accuracy: 0.860 itemLoss: 0.335\n","[5,    15] loss: 0.515 accuracy: 0.800 itemLoss: 0.490\n","[5,    20] loss: 0.315 accuracy: 0.880 itemLoss: 0.325\n","[5,    25] loss: 0.426 accuracy: 0.920 itemLoss: 0.213\n","[5,    30] loss: 0.505 accuracy: 0.840 itemLoss: 0.386\n","[5,    35] loss: 0.407 accuracy: 0.860 itemLoss: 0.457\n","[5,    40] loss: 0.385 accuracy: 0.800 itemLoss: 0.418\n","Accuracy of the network on the validation images: 67 %\n","Finished Training\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BN7tvagIZLcN","colab_type":"code","outputId":"870ea390-3d41-4504-f2c1-9156536e08a9","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1574844376360,"user_tz":480,"elapsed":739218,"user":{"displayName":"Khanh Nguyen","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAhmdzYE4Np2rwuKzC_ZuN37tvzMf_2DpvMfuRD=s64","userId":"04394352403186034085"}}},"source":["model = torchvision.models.resnet34(pretrained=True)\n","model.cuda()\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n","\n","train_losses, train_accs = [], []\n","val_losses, val_accs = [], []\n","\n","for epoch in range(10):  # loop over the dataset multiple times\n","\n","    running_loss = 0.0\n","    model.train()\n","    for i, data in enumerate(train_loader, 0):\n","        # get the inputs; data is a list of [inputs, labels]\n","        inputs, labels = data\n","        inputs = inputs.cuda()\n","        labels = labels.cuda()\n","\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs = model(inputs)\n","\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        pred = outputs.data.max(1)[1]\n","        matches = labels == pred\n","        accuracy = matches.float().mean()\n","\n","        # print statistics\n","        running_loss += loss.item()\n","        if i % minibatches == minibatches - 1:    # print every n mini-batches\n","            print('[%d, %5d] loss: %.3f accuracy: %.3f itemLoss: %.3f' %\n","                  (epoch + 1, i + 1, running_loss / minibatches, accuracy, loss.item()))\n","            running_loss = 0.0\n","\n","        train_losses.append(loss.item())\n","        train_accs.append(accuracy.item())\n","    \n","    model.eval()\n","    val_loss, correct = 0., 0\n","    total = 0\n","    with torch.no_grad():\n","      for data in val_loader:\n","        inputs, labels = data\n","        inputs = inputs.cuda()\n","        labels = labels.cuda()\n","        outputs = model(inputs)\n","        val_loss += criterion(outputs, labels).item()\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","    print('Accuracy of the network on the validation images: %d %%' % (\n","    100 * correct / total))\n","\n","    val_loss /= len(val_loader)\n","    acc = correct / len(val_loader)\n","\n","    val_losses.append(val_loss)\n","    val_accs.append(acc)\n","\n","print('Finished Training')"],"execution_count":17,"outputs":[{"output_type":"stream","text":["[1,     5] loss: 6.957 accuracy: 0.280 itemLoss: 4.763\n","[1,    10] loss: 3.008 accuracy: 0.440 itemLoss: 2.231\n","[1,    15] loss: 1.452 accuracy: 0.640 itemLoss: 1.265\n","[1,    20] loss: 0.949 accuracy: 0.700 itemLoss: 1.151\n","[1,    25] loss: 0.858 accuracy: 0.740 itemLoss: 0.869\n","[1,    30] loss: 0.840 accuracy: 0.740 itemLoss: 0.902\n","[1,    35] loss: 0.698 accuracy: 0.820 itemLoss: 0.599\n","[1,    40] loss: 0.622 accuracy: 0.680 itemLoss: 0.685\n","Accuracy of the network on the validation images: 77 %\n","[2,     5] loss: 0.521 accuracy: 0.920 itemLoss: 0.335\n","[2,    10] loss: 0.548 accuracy: 0.760 itemLoss: 0.775\n","[2,    15] loss: 0.532 accuracy: 0.720 itemLoss: 0.693\n","[2,    20] loss: 0.508 accuracy: 0.840 itemLoss: 0.421\n","[2,    25] loss: 0.527 accuracy: 0.860 itemLoss: 0.438\n","[2,    30] loss: 0.479 accuracy: 0.780 itemLoss: 0.592\n","[2,    35] loss: 0.370 accuracy: 0.800 itemLoss: 0.427\n","[2,    40] loss: 0.395 accuracy: 0.860 itemLoss: 0.400\n","Accuracy of the network on the validation images: 87 %\n","[3,     5] loss: 0.430 accuracy: 0.920 itemLoss: 0.293\n","[3,    10] loss: 0.338 accuracy: 0.860 itemLoss: 0.357\n","[3,    15] loss: 0.388 accuracy: 0.840 itemLoss: 0.324\n","[3,    20] loss: 0.324 accuracy: 0.860 itemLoss: 0.323\n","[3,    25] loss: 0.225 accuracy: 0.980 itemLoss: 0.137\n","[3,    30] loss: 0.334 accuracy: 0.880 itemLoss: 0.382\n","[3,    35] loss: 0.344 accuracy: 0.880 itemLoss: 0.392\n","[3,    40] loss: 0.430 accuracy: 0.840 itemLoss: 0.362\n","Accuracy of the network on the validation images: 85 %\n","[4,     5] loss: 0.257 accuracy: 0.900 itemLoss: 0.273\n","[4,    10] loss: 0.282 accuracy: 1.000 itemLoss: 0.106\n","[4,    15] loss: 0.308 accuracy: 0.940 itemLoss: 0.184\n","[4,    20] loss: 0.176 accuracy: 0.960 itemLoss: 0.199\n","[4,    25] loss: 0.322 accuracy: 0.880 itemLoss: 0.356\n","[4,    30] loss: 0.288 accuracy: 0.900 itemLoss: 0.201\n","[4,    35] loss: 0.271 accuracy: 0.920 itemLoss: 0.213\n","[4,    40] loss: 0.228 accuracy: 0.940 itemLoss: 0.165\n","Accuracy of the network on the validation images: 92 %\n","[5,     5] loss: 0.320 accuracy: 0.880 itemLoss: 0.272\n","[5,    10] loss: 0.227 accuracy: 0.940 itemLoss: 0.242\n","[5,    15] loss: 0.151 accuracy: 0.980 itemLoss: 0.088\n","[5,    20] loss: 0.208 accuracy: 0.960 itemLoss: 0.152\n","[5,    25] loss: 0.204 accuracy: 0.980 itemLoss: 0.091\n","[5,    30] loss: 0.224 accuracy: 0.940 itemLoss: 0.140\n","[5,    35] loss: 0.166 accuracy: 0.900 itemLoss: 0.281\n","[5,    40] loss: 0.215 accuracy: 0.880 itemLoss: 0.231\n","Accuracy of the network on the validation images: 89 %\n","[6,     5] loss: 0.206 accuracy: 0.940 itemLoss: 0.170\n","[6,    10] loss: 0.156 accuracy: 0.980 itemLoss: 0.121\n","[6,    15] loss: 0.170 accuracy: 0.960 itemLoss: 0.111\n","[6,    20] loss: 0.188 accuracy: 0.960 itemLoss: 0.203\n","[6,    25] loss: 0.293 accuracy: 0.760 itemLoss: 0.496\n","[6,    30] loss: 0.259 accuracy: 0.820 itemLoss: 0.350\n","[6,    35] loss: 0.136 accuracy: 1.000 itemLoss: 0.046\n","[6,    40] loss: 0.174 accuracy: 0.920 itemLoss: 0.190\n","Accuracy of the network on the validation images: 92 %\n","[7,     5] loss: 0.137 accuracy: 0.940 itemLoss: 0.158\n","[7,    10] loss: 0.152 accuracy: 0.980 itemLoss: 0.089\n","[7,    15] loss: 0.119 accuracy: 0.980 itemLoss: 0.116\n","[7,    20] loss: 0.089 accuracy: 0.980 itemLoss: 0.087\n","[7,    25] loss: 0.160 accuracy: 0.960 itemLoss: 0.109\n","[7,    30] loss: 0.136 accuracy: 0.920 itemLoss: 0.166\n","[7,    35] loss: 0.171 accuracy: 0.960 itemLoss: 0.133\n","[7,    40] loss: 0.186 accuracy: 0.940 itemLoss: 0.155\n","Accuracy of the network on the validation images: 93 %\n","[8,     5] loss: 0.127 accuracy: 0.980 itemLoss: 0.122\n","[8,    10] loss: 0.093 accuracy: 0.980 itemLoss: 0.058\n","[8,    15] loss: 0.129 accuracy: 0.940 itemLoss: 0.189\n","[8,    20] loss: 0.149 accuracy: 1.000 itemLoss: 0.062\n","[8,    25] loss: 0.110 accuracy: 0.920 itemLoss: 0.170\n","[8,    30] loss: 0.118 accuracy: 0.940 itemLoss: 0.149\n","[8,    35] loss: 0.124 accuracy: 0.940 itemLoss: 0.255\n","[8,    40] loss: 0.116 accuracy: 0.980 itemLoss: 0.121\n","Accuracy of the network on the validation images: 93 %\n","[9,     5] loss: 0.085 accuracy: 1.000 itemLoss: 0.033\n","[9,    10] loss: 0.109 accuracy: 0.960 itemLoss: 0.083\n","[9,    15] loss: 0.128 accuracy: 0.980 itemLoss: 0.036\n","[9,    20] loss: 0.134 accuracy: 0.960 itemLoss: 0.088\n","[9,    25] loss: 0.144 accuracy: 0.920 itemLoss: 0.197\n","[9,    30] loss: 0.133 accuracy: 0.980 itemLoss: 0.064\n","[9,    35] loss: 0.101 accuracy: 0.980 itemLoss: 0.095\n","[9,    40] loss: 0.077 accuracy: 0.980 itemLoss: 0.058\n","Accuracy of the network on the validation images: 93 %\n","[10,     5] loss: 0.074 accuracy: 0.980 itemLoss: 0.071\n","[10,    10] loss: 0.083 accuracy: 1.000 itemLoss: 0.024\n","[10,    15] loss: 0.098 accuracy: 0.940 itemLoss: 0.179\n","[10,    20] loss: 0.085 accuracy: 1.000 itemLoss: 0.038\n","[10,    25] loss: 0.097 accuracy: 0.940 itemLoss: 0.136\n","[10,    30] loss: 0.098 accuracy: 0.980 itemLoss: 0.092\n","[10,    35] loss: 0.084 accuracy: 1.000 itemLoss: 0.030\n","[10,    40] loss: 0.059 accuracy: 0.980 itemLoss: 0.056\n","Accuracy of the network on the validation images: 93 %\n","Finished Training\n"],"name":"stdout"}]}]}