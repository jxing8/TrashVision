{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FullNet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5O7DKoWajeW",
        "colab_type": "text"
      },
      "source": [
        "# FullNet.ipynb\n",
        "\n",
        "This file was used to train the single model that used both the TrashNet and the Kaggle data. This was used as a point of comparison for our Two-CNN method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPj_HZs8afH5",
        "colab_type": "code",
        "outputId": "c0b302d7-69ba-443b-f0cb-24bdd844f32f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "# HYPERPARAMETERS\n",
        "train_batch_size = 64\n",
        "test_batch_size = 64\n",
        "train_validation_split = 4/5\n",
        "log_interval = 20\n",
        "lr = 0.01\n",
        "momentum = 0.9\n",
        "num_epochs = 5\n",
        "\n",
        "def prepare_data():\n",
        "    training_path = '/content/drive/My Drive/Colab Notebooks/Data/TrashNet_Data/'\n",
        "    # use the RandomCrop to make all images the same size\n",
        "    # this gave much better results than transforms.Resize()\n",
        "    trainval_set = torchvision.datasets.ImageFolder(root=training_path, transform=transforms.Compose([transforms.RandomCrop([256,256],pad_if_needed=True), transforms.ToTensor()]))\n",
        "    trainval_size = len(trainval_set)\n",
        "    train_size = int(trainval_size * train_validation_split)\n",
        "\n",
        "    train_set, val_set = torch.utils.data.random_split(trainval_set, [train_size, trainval_size-train_size])\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_set,batch_size=train_batch_size,num_workers=0,shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_set,batch_size=test_batch_size,num_workers=0,shuffle=True)\n",
        "\n",
        "    return train_set, val_set, train_loader, val_loader\n",
        "\n",
        "def train(model, criterion, train_loader, optimizer, device):\n",
        "    model.train() # set the mode of the model to training\n",
        "    total_loss = 0\n",
        "    for i, data in enumerate(train_loader):\n",
        "        imgs, lbls = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, lbls)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        if (i+1) % log_interval == 0:\n",
        "            mean_loss = total_loss / log_interval\n",
        "            print('\\tbatch {:4d}: loss={:.3f}'.format(i+1, mean_loss))\n",
        "            total_loss = 0.\n",
        "\n",
        "def test(model, test_loader, device):\n",
        "    model.eval()\n",
        "    total, correct = 0, 0\n",
        "    T0, F0, T1, F1 = 0, 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            imgs, lbls = data[0].to(device), data[1].to(device)\n",
        "            outputs = model(imgs)\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            total += lbls.shape[0]\n",
        "            correct += (preds == lbls).sum().item()\n",
        "\n",
        "            T0 += (preds[preds == lbls] == 0).sum().item()\n",
        "            F0 += (preds[preds != lbls] == 0).sum().item()\n",
        "            T1 += (preds[preds == lbls] == 1).sum().item()\n",
        "            F1 += (preds[preds != lbls] == 1).sum().item()\n",
        "\n",
        "    acc = correct / total\n",
        "    # confusion = np.array([[T0, F0], [F1, T1]])\n",
        "    print('\\tacc={:.3f}'.format(acc))\n",
        "    # print(confusion)\n",
        "    \n",
        "\n",
        "def main():\n",
        "    device = torch.device('cpu')\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda:0')\n",
        "    print('Using {}'.format(device))\n",
        "\n",
        "    train_set, val_set, train_loader, val_loader = prepare_data()\n",
        "    print('Training set size: {}'.format(len(train_set)))\n",
        "    print('Validation set size: {}'.format(len(val_set)))\n",
        "\n",
        "    model = models.resnet34(pretrained=True)\n",
        "    # model = torch.load('full_model.pt')\n",
        "    model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(\n",
        "        model.parameters(),\n",
        "        lr = lr,\n",
        "        momentum = momentum\n",
        "    )\n",
        "\n",
        "    for e in range(num_epochs):\n",
        "        print('Training epoch {}'.format(e))\n",
        "        train(model, criterion, train_loader, optimizer, device)\n",
        "        print('Testing on validation set')\n",
        "        test(model, val_loader, device)\n",
        "\n",
        "    torch.save(model, 'full_model.pt')\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "Using cuda:0\n",
            "Training set size: 12844\n",
            "Validation set size: 3212\n",
            "Training epoch 0\n",
            "\tbatch   20: loss=1.330\n",
            "\tbatch   40: loss=0.232\n",
            "\tbatch   60: loss=0.256\n",
            "\tbatch   80: loss=0.232\n",
            "\tbatch  100: loss=0.218\n",
            "\tbatch  120: loss=0.161\n",
            "\tbatch  140: loss=0.192\n",
            "\tbatch  160: loss=0.162\n",
            "\tbatch  180: loss=0.184\n",
            "\tbatch  200: loss=0.156\n",
            "Testing on validation set\n",
            "\tacc=0.935\n",
            "Training epoch 1\n",
            "\tbatch   20: loss=0.142\n",
            "\tbatch   40: loss=0.151\n",
            "\tbatch   60: loss=0.162\n",
            "\tbatch   80: loss=0.149\n",
            "\tbatch  100: loss=0.116\n",
            "\tbatch  120: loss=0.115\n",
            "\tbatch  140: loss=0.126\n",
            "\tbatch  160: loss=0.115\n",
            "\tbatch  180: loss=0.123\n",
            "\tbatch  200: loss=0.105\n",
            "Testing on validation set\n",
            "\tacc=0.957\n",
            "Training epoch 2\n",
            "\tbatch   20: loss=0.137\n",
            "\tbatch   40: loss=0.106\n",
            "\tbatch   60: loss=0.106\n",
            "\tbatch   80: loss=0.127\n",
            "\tbatch  100: loss=0.101\n",
            "\tbatch  120: loss=0.080\n",
            "\tbatch  140: loss=0.121\n",
            "\tbatch  160: loss=0.099\n",
            "\tbatch  180: loss=0.114\n",
            "\tbatch  200: loss=0.101\n",
            "Testing on validation set\n",
            "\tacc=0.964\n",
            "Training epoch 3\n",
            "\tbatch   20: loss=0.083\n",
            "\tbatch   40: loss=0.091\n",
            "\tbatch   60: loss=0.073\n",
            "\tbatch   80: loss=0.075\n",
            "\tbatch  100: loss=0.092\n",
            "\tbatch  120: loss=0.092\n",
            "\tbatch  140: loss=0.091\n",
            "\tbatch  160: loss=0.053\n",
            "\tbatch  180: loss=0.060\n",
            "\tbatch  200: loss=0.097\n",
            "Testing on validation set\n",
            "\tacc=0.968\n",
            "Training epoch 4\n",
            "\tbatch   20: loss=0.075\n",
            "\tbatch   40: loss=0.059\n",
            "\tbatch   60: loss=0.074\n",
            "\tbatch   80: loss=0.080\n",
            "\tbatch  100: loss=0.061\n",
            "\tbatch  120: loss=0.082\n",
            "\tbatch  140: loss=0.085\n",
            "\tbatch  160: loss=0.074\n",
            "\tbatch  180: loss=0.063\n",
            "\tbatch  200: loss=0.083\n",
            "Testing on validation set\n",
            "\tacc=0.964\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ResNet. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Conv2d. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BatchNorm2d. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type MaxPool2d. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Sequential. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BasicBlock. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type AdaptiveAvgPool2d. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXVckMD0gltp",
        "colab_type": "code",
        "outputId": "eb759f4d-ffba-46fb-d151-a6f325d1f55e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2018 NVIDIA Corporation\n",
            "Built on Sat_Aug_25_21:08:01_CDT_2018\n",
            "Cuda compilation tools, release 10.0, V10.0.130\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}