{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"texturerecognitionOne.ipynb","provenance":[{"file_id":"1W6DI5WeP8ko6DNaIdZpT-hgB5di9_U8O","timestamp":1574872531152}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"JsH69sBEVU7h","colab_type":"code","outputId":"ca226f1d-ff90-48ee-831f-d35c4178d82c","executionInfo":{"status":"ok","timestamp":1574873084947,"user_tz":480,"elapsed":24140,"user":{"displayName":"Khanh Nguyen","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAhmdzYE4Np2rwuKzC_ZuN37tvzMf_2DpvMfuRD=s64","userId":"04394352403186034085"}},"colab":{"base_uri":"https://localhost:8080/","height":125}},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0Lf4XhZrLFxv","colab_type":"code","colab":{}},"source":["!unzip -q drive/'My Drive'/dataset-original.zip"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"v-IQewQhBOfA","colab_type":"code","colab":{}},"source":["!mkdir dataset-validation\n","!mkdir dataset-validation/cardboard\n","!mkdir dataset-validation/glass\n","!mkdir dataset-validation/paper\n","!mkdir dataset-validation/plastic\n","!mkdir dataset-validation/metal\n","!mkdir dataset-validation/trash\n","!ls\n","!ls dataset-original/trash"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vK4hhHg0FNiC","colab_type":"code","colab":{}},"source":["!shuf -n 101 -e dataset-original/glass/* | xargs -i mv {} dataset-validation/glass/\n","!shuf -n 144 -e dataset-original/paper/* | xargs -i mv {} dataset-validation/paper/\n","!shuf -n 53 -e dataset-original/cardboard/* | xargs -i mv {} dataset-validation/cardboard/\n","!shuf -n 82 -e dataset-original/plastic/* | xargs -i mv {} dataset-validation/plastic/\n","!shuf -n 60 -e dataset-original/metal/* | xargs -i mv {} dataset-validation/metal/\n","!shuf -n 37 -e dataset-original/trash/* | xargs -i mv {} dataset-validation/trash/"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uFV1fTT9HspD","colab_type":"code","colab":{}},"source":["!ls dataset-original/glass | wc -l\n","!ls dataset-validation/glass | wc -l"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VCMDyL_T7fdA","colab_type":"code","colab":{}},"source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","import torch.nn as nn\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"i6wBY7KQYiTN","colab_type":"code","colab":{}},"source":["transformationsTrain = transforms.Compose([transforms.Resize(255), transforms.RandomHorizontalFlip(), transforms.RandomVerticalFlip(),\n","                                           transforms.RandomRotation(degrees = (90, -90)), transforms.ToTensor(), \n","                                           transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","transformationsValidation = transforms.Compose([transforms.Resize(255), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zDud-z_t-7QK","colab_type":"code","colab":{}},"source":["train_set = datasets.ImageFolder(\"dataset-original\", transform = transformationsTrain)\n","val_set = datasets.ImageFolder(\"dataset-validation\", transform = transformationsValidation)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ICFzgBLtdiwC","colab_type":"code","colab":{}},"source":["train_loader = torch.utils.data.DataLoader(train_set, batch_size=50, shuffle=True)\n","val_loader = torch.utils.data.DataLoader(val_set, batch_size =50, shuffle=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"u3TUtmB4LheG","colab_type":"code","colab":{}},"source":["model = torchvision.models.resnet34(pretrained=True)\n","model.cuda()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SWIqG7BNLuXi","colab_type":"code","colab":{}},"source":["learningRate = 1e-3\n","epochs = 10\n","minibatches = 5"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aFM3s_k62J7F","colab_type":"code","colab":{}},"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), learningRate)\n","\n","train_losses, train_accs = [], []\n","val_losses, val_accs = [], []\n","\n","for epoch in range(epochs):  # loop over the dataset multiple times\n","\n","    running_loss = 0.0\n","    model.train()\n","    for i, data in enumerate(train_loader, 0):\n","        # get the inputs; data is a list of [inputs, labels]\n","        inputs, labels = data\n","        inputs = inputs.cuda()\n","        labels = labels.cuda()\n","\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs = model(inputs)\n","\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        pred = outputs.data.max(1)[1]\n","        matches = labels == pred\n","        accuracy = matches.float().mean()\n","\n","        # print statistics\n","        running_loss += loss.item()\n","        if i % minibatches == minibatches - 1:    # print every n mini-batches\n","            print('[%d, %5d] loss: %.3f accuracy: %.3f itemLoss: %.3f' %\n","                  (epoch + 1, i + 1, running_loss / minibatches, accuracy, loss.item()))\n","            running_loss = 0.0\n","\n","        train_losses.append(loss.item())\n","        train_accs.append(accuracy.item())\n","    \n","    model.eval()\n","    val_loss, correct = 0., 0\n","    total = 0\n","    with torch.no_grad():\n","      for data in val_loader:\n","        inputs, labels = data\n","        inputs = inputs.cuda()\n","        labels = labels.cuda()\n","        outputs = model(inputs)\n","        val_loss += criterion(outputs, labels).item()\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","    print('Accuracy of the network on the validation images: %d %%' % (\n","    100 * correct / total))\n","\n","    val_loss /= len(val_loader)\n","    acc = correct / len(val_loader)\n","\n","    val_losses.append(val_loss)\n","    val_accs.append(acc)\n","\n","print('Finished Training')"],"execution_count":0,"outputs":[]}]}